---
title: "Final Project - Police Versus Pizza Response Times"
author: "Neil Stein, GitHub:neil-stein & Mitch Bobbin, GitHub:mbobbin"
format: html
execute:
  echo: false
  eval: false

---

# initial data testing
```{python}
import pandas as pd
import zipfile

data_path = r"calls_all.csv.zip"
with zipfile.ZipFile(data_path, "r") as zip_ref:
    with zip_ref.open("calls_all.csv") as file:
        new_data = pd.read_csv(file)
```

# Data Structure
```{python}
new_data.shape
new_data.columns
new_data["Event Clearance Description"].unique()
new_data["Call Type"].unique()
new_data["Priority"].unique()
new_data["Initial Call Type"].nunique()
new_data["Final Call Type"].nunique()

new_data["Precinct"].unique()
len(new_data[new_data["Precinct"]=="UNKNOWN"])

new_data["Sector"].unique()
new_data["Beat"].unique()

```

5,926,156 unique calls (rows)
13 colums for each observation

Event Clearance Description appears to indicate what an officer did to resolve a phone call.

Call Type is nominal but unclear what the dispositions are for some.
Priority is a quantitative variable from 1-9. has 712 nas also.

Initial Call Type is very messy and we could recode. lots of different dispositions that we could aggregate into common categories. Same with Final Call Type. 326 and 436 respectively unique values.

West, Southwest, East, South, North, and Unknown are the only values for Precinct. There's 52,381 Unknown entries - not a lot.

Sector has a number of values that seem to be names? Need to investigate what this means if a data dictionary is handy.

Quite a few beats also.

```{python}
import altair as alt
grouping_911 = new_data.groupby("Call Type").size().reset_index(name="count")

alt.Chart(grouping_911).mark_bar().encode(
    alt.X("Call Type"),
    alt.Y("count")
).properties(title="Number of Each Call Types in Seattle")

```


# testing date-time range
```{python}
new_data["Original Time Queued"] = pd.to_datetime(new_data["Original Time Queued"], 
    format="%m/%d/%Y %I:%M:%S %p", 
    utc=True)

earliest_time = new_data["Original Time Queued"].min()
latest_time = new_data["Original Time Queued"].max()

print(f"The earliest time is {earliest_time}, the latest is {latest_time}")
```

# initial data clean up
```{python}
import pandas as pd

# removing rows with blurred lat-long that is not usable
new_data = new_data[new_data["Blurred_Longitude"] != -1]
new_data = new_data[new_data["Blurred_Longitude"] != 0]

# removing rows without an arrival time
new_data = new_data.dropna(subset= ["Arrived Time"])

# filtering down to the list of beats that are in Seattle city limits (per their website)
seattle_beats = pd.read_csv("Seattle_PD_beats.csv")
seattle_beats_list = seattle_beats["beat"].unique()

new_data = new_data[new_data["Beat"].isin(seattle_beats_list)]
```

# creating new data - time differences
```{python}
import pandas as pd

# time difference calculation
new_data["Arrived Time"] = pd.to_datetime(
    new_data["Arrived Time"], 
    format="%m/%d/%Y %I:%M:%S %p", 
    utc=True
)

new_data["Original Time Queued"] = pd.to_datetime(new_data["Original Time Queued"], format="%m/%d/%Y %I:%M:%S %p", utc= True)

new_data["Time_Diff_Minutes"] = (new_data["Arrived Time"] - new_data["Original Time Queued"]).dt.total_seconds()/60
new_data["Time_Diff_Minutes"] = new_data["Time_Diff_Minutes"].fillna(0)
```

# checking general average response time
```{python}
average_response_time = new_data["Time_Diff_Minutes"].mean()
print(f"average response time overall is {average_response_time: .2f} minutes")
```

# checking by call type
```{python}
grouping_911 = new_data.groupby("Call Type")
emergency_response_time = grouping_911.get_group("911")["Time_Diff_Minutes"].mean()
print(f"average response time for 911 calls is {emergency_response_time: .2f} minutes")
```

# average response time by location

```{python}
import altair as alt
#look at which beats have the most calls
beat_counts=new_data.groupby("Beat").size().sort_values(ascending=False).reset_index(name="count")

#create a new df looking at avg response time by beats
response_beat=new_data.groupby("Beat")["Time_Diff_Minutes"].mean(

).sort_values(
    ascending=False).reset_index()
#create a chart
alt.Chart(response_beat).mark_bar().encode(
    alt.X("Beat",sort=response_beat["Beat"].tolist()),
    alt.Y("Time_Diff_Minutes",title="Total Response Time")
).properties(title="Overall Response Time By Beat")

#now look at just 911 calls
calls_911=new_data[new_data["Call Type"]=="911"]
#groupby beat to see what avg response time is per beat
response_beat_911=calls_911.groupby("Beat")["Time_Diff_Minutes"].mean(

).sort_values(
    ascending=False).reset_index()
#going to need this later to create a chart displaying the 
#avg response time
emergency_response_time_df = pd.DataFrame(
    {'emergency_response_time': [emergency_response_time]})

#create initial chart for 911 calls by beat, and response time
chart_beat_911=alt.Chart(response_beat_911).mark_bar().encode(
    alt.X("Beat",sort=response_beat_911["Beat"].tolist()),
    alt.Y("Time_Diff_Minutes",title="Response Time (Minutes)")
).properties(title="Response Time By Beat for 911 Calls")

#create the horizontal line representing avg response time for 
#911 calls regardless of beat
avg_response_line = alt.Chart(emergency_response_time_df).mark_rule(color="red").encode(
    y=alt.Y("emergency_response_time:Q", title="Average Response Time")  # Quantitative data type
)

chart_w_average=chart_beat_911+avg_response_line
chart_w_average.show()

```

Final Chart shows that B1, B2, B3, U3, J2, J3, J1, N2, D3, U1, Q2, N3, Q1,U2, C2, D2, L3, L2 all perform worse on response time than the average.


# average response time by call type


# grouping initial call type for 911 calls 
There are 249 unique values for this. It would be nice if we could group in a way that provides some clarity about what kind of calls are receiving fast response and which ones are not. Then we can evaluate if its proportionate to the crime, and we could provide granularity. Maybe even use to filter a map on a dashboard by the response times based upon our new call types
```{python}
unique_call_types=calls_911["Initial Call Type"].unique()

calls_911.groupby("Initial Call Type").size().sort_values(ascending=False).reset_index(name="count").head(20)

#create new initial call category
import numpy as np

# Define conditions and corresponding labels
conditions = [
    calls_911["Initial Call Type"].str.contains("DV", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("narcotics", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("Theft|property|Burg|Robbery|Trespass|Arson", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("ASLT|fight|bomb|shots", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("Nuisance|Noise", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("suicide|Wellness Check|Welfare Check|Overdose|emotional crisis", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("Suspicious", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("MVC|Traffic|DUI", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("lewd|Rape|Sex", case=False, na=False)
]

labels = [
    "domestic violence",
    "drug crime",
    "property crime",
    "violent crime",
    "noise complaint",
    "wellness check",
    "community suspicion",
    "traffic safety",
    "sex crime"
]

# Apply transformations using np.select
calls_911["Call Category"] = np.select(conditions, labels, default="other")

check_call_cat=calls_911[["Call Category","Initial Call Type"]]


# Calculate Q1, Q3, and IQR
q1 = calls_911['Time_Diff_Minutes'].quantile(0.25)
q3 = calls_911['Time_Diff_Minutes'].quantile(0.75)
iqr = q3 - q1

# Define the acceptable range
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

print(f"Lower bound: {lower_bound}, Upper bound: {upper_bound}")

filtered_calls_911 = calls_911[
    (calls_911['Time_Diff_Minutes'] >= lower_bound) &
    (calls_911['Time_Diff_Minutes'] <= upper_bound)
]

print(f"Original data size: {len(calls_911)}, Filtered data size: {len(filtered_calls_911)}")


call_category_chart2 = alt.Chart(calls_911).mark_bar().encode(
    x=alt.X(
        'Call Category:N',
        sort='-y',  # Sort based on the y-axis values in descending order
        title="Call Category"
    ),
    y=alt.Y(
        'mean(Time_Diff_Minutes):Q',
        title="Average Response Time (Minutes)"
    )
).properties(
    title="Average Police Response Time by Call Category in Seattle"
)

call_category_chart2.show()
#Now I want to transpose this over census, and geographic data
```

# Transposing over geographic data
```{python}
import geopandas as gpd

shapefile_path = r"Seattle_PD_beats.zip"

# Load the shapefile
gdf = gpd.read_file(f"zip://{shapefile_path}")

gdf = gdf.rename(columns={'beat': 'Beat'})

calls_911_agg=calls_911.groupby(["Beat","Call Category"]).mean("Time_Diff_Minutes").reset_index()

shapefile_path = r"census_seattle.zip"

# Read the shapefile from the zip
gdf_census = gpd.read_file(f"zip://{shapefile_path}")
from shapely.geometry import Point

# Convert calls_911 to a GeoDataFrame
calls_911['geometry'] = calls_911.apply(lambda row: Point(row['Blurred_Longitude'], row['Blurred_Latitude']), axis=1)
calls_911_geo = gpd.GeoDataFrame(calls_911, geometry='geometry')

# Set the CRS to match the latitude/longitude system (WGS 84)
calls_911_geo = calls_911_geo.set_crs("EPSG:4326")


# Filter columns with '20' in their names or the 'geometry' column
# Get the first 4 columns
first_columns = gdf_census.iloc[:, :2]

# Get the last 32 columns
last_columns = gdf_census.iloc[:, -33:]

# Combine them into a new DataFrame
gdf_census_filtered = pd.concat([first_columns, last_columns], axis=1)

# Verify the result
print(gdf_census_filtered.head())

gdf_census_filtered = gdf_census_filtered.rename(columns={
    "TRACT_20_N": "tract_number",
    "F2020_PL_d": "total_population",
    "F2020_PL_1":"one_race_pop",
    "F2020_PL_2":"white_pop",
    "F2020_PL_3":"black_pop",
    "F2020_PL_4":"native_am_alaskan_pop",
    "F2020_PL_5":"asian_pop",
    "F2020_PL_6":"pacific_island_pop",
    "F2020_PL_7":"other_race_pop",
    "F2020_PL_8":"two_races_pop",
    "F2020_PL_9":"latino_pop"

})

# Dictionary of column mappings
# List of demographic columns to calculate percentages for
demographic_columns = [
    "one_race_pop",
    "white_pop",
    "black_pop",
    "native_am_alaskan_pop",
    "asian_pop",
    "pacific_island_pop",
    "other_race_pop",
    "two_races_pop",
    "latino_pop"
]

# Calculate percentage for each column and create a new column
for col in demographic_columns:
    gdf_census_filtered[f"{col}_perc"] = gdf_census_filtered[col] / gdf_census_filtered["total_population"]

# Verify the result
print(gdf_census_filtered.head())

#now spatial join on our 911 calls df. I want to assign a census tract to each call based on if it intersects with our census geometries. Then, i'll group by census tract, and recover each tracts response time.

# Ensure gdf_census_filtered is a GeoDataFrame and set the geometry
gdf_census_filtered = gpd.GeoDataFrame(gdf_census_filtered, geometry="geometry")

# Now, check the CRS of both GeoDataFrames
print("CRS of gdf_census_filtered:", gdf_census_filtered.crs)
print("CRS of calls_911_geo:", calls_911_geo.crs)

# If they are different, reproject one of them
if gdf_census_filtered.crs != calls_911_geo.crs:
    gdf_census_filtered = gdf_census_filtered.to_crs(calls_911_geo.crs)

# Perform the spatial join
census_tracts_gdf = gdf_census_filtered[["geometry", "tract_number"]]

# Perform the spatial join
calls_with_census = gpd.sjoin(calls_911_geo, census_tracts_gdf, how="left", predicate="intersects")

# Verify the result
print(calls_with_census.head())

#now groupby census_tract:

agg_calls_census=calls_with_census.groupby("tract_number")["Time_Diff_Minutes"].mean().reset_index()

#now add the geometry by merging the df with the original census:

# Merge the aggregated data with the original census GeoDataFrame
agg_calls_census_merged = pd.merge(gdf_census_filtered, agg_calls_census, 
                                   how="left", 
                                   on="tract_number")

# Ensure the merged result is a GeoDataFrame
agg_calls_census_merged = gpd.GeoDataFrame(agg_calls_census_merged, geometry="geometry")

# Check the result
print(agg_calls_census_merged["Time_Diff_Minutes"].head())

#now plot using altair:

agg_calls_census_merged = agg_calls_census_merged.to_crs("EPSG:4326")

# Convert the geometries to GeoJSON format
agg_calls_census_merged['geojson'] = agg_calls_census_merged.geometry.apply(lambda x: x.__geo_interface__)

# Create an Altair chart with the geometries and color by 'Time_Diff_Minutes'
chart = alt.Chart(agg_calls_census_merged).mark_geoshape().encode(
    color='Time_Diff_Minutes:Q',  # Quantitative encoding for the color scale
    tooltip=['tract_number:N', 'Time_Diff_Minutes:Q']  # Tooltip with relevant information
).properties(
    width=600,
    height=400
)

# Show the chart
chart

```

# create bones for shiny app:

```{python}
#spatial join the 911 calls geoframe to census tracts:
calls_with_census=gpd.sjoin(calls_911_geo,census_tracts_gdf,predicate="intersects",how="left")


calls_with_census_grouped = (
    calls_with_census
    .groupby(["tract_number", "Call Category"])
    .agg(
        Time_Diff_Minutes=("Time_Diff_Minutes", "mean"),  # Calculate the mean
        Num_Observations=("Time_Diff_Minutes", "size")  # Count the number of observations
    )
    .reset_index()  # Reset index to get a clean dataframe
)
#drop the useless columns:
calls_with_census_grouped=calls_with_census_grouped[["tract_number","Call Category","Time_Diff_Minutes","Num_Observations"]]

#join using tract number 

calls_type_census = pd.merge(gdf_census_filtered, calls_with_census_grouped, 
                                   how="left", 
                                   on="tract_number")

calls_type_census.plot(column="Time_Diff_Minutes")

calls_type_census.to_csv("response_times_by_tract_call_type.csv")
viol_crime=calls_type_census[calls_type_census["Call Category"]=="violent crime"]

viol_crime.plot(column="Time_Diff_Minutes")

agg_calls_census_merged.to_csv("agg_calls_census_merged.csv")

avg_citywide_by_category=calls_911.groupby("Call Category").mean("Time_Diff_Minutes").reset_index()
avg_citywide_by_category=avg_citywide_by_category[["Call Category","Time_Diff_Minutes"]]

avg_citywide_by_category.to_csv("avg_citywide_by_category.csv")
```

# number of observations by census tract

```{python}
calls_per_tract=calls_type_census.groupby("tract_number").agg(
        Time_Diff_Minutes=("Time_Diff_Minutes", "mean"),  # Calculate the mean
        Num_Observations=("Num_Observations", "sum")  # Count the number of observations
    ).reset_index()
calls_per_tract_geo=pd.merge(calls_per_tract,census_tracts_gdf,how="left",on="tract_number")
calls_per_tract_geo=gpd.GeoDataFrame(calls_per_tract_geo)

calls_per_tract_geo.plot(column="Num_Observations",
legend=True,
legend_kwds={"label":"Number of Calls",
"orientation":"vertical"},
cmap="Reds")
calls_per_tract["Num_Observations"].min()
calls_per_tract["Num_Observations"].max()
calls_per_tract["Num_Observations"].mean()
calls_per_tract["Num_Observations"].median()

histogram = (
    alt.Chart(calls_per_tract)
    .transform_filter('datum.Num_Observations < 25000')  # Filter rows with less than 25,000 calls
    .mark_bar()
    .encode(
        x=alt.X('Num_Observations', bin=True, title='Number of Calls'),  # Bin the x-axis
        y=alt.Y('count()', title='Count')  # Count occurrences
    )
    .properties(
        title="Histogram of Tracts with Less Than 25,000 Calls"
    )
)


```

# NLP Extra Credit - Name-based Analysis
Since our dataset from the city of Seattle on food-based businesses does not come with any kind of attribution for these venues, we will be using Natural Language Processing to isolate the food venues likely to serve pizzas. 

```{python}
import spacy
import pandas as pd

# setup steps
restaurant_df = pd.read_csv("Businesses_Food-King_County_20241024.csv")
nlp = spacy.load("it_core_news_sm")
# common pizza/Italian restaurant key words and top 20 most common Italian last names (source: https://surnamesinitaly.com/italian-surnames-usa/)
ital_keywords = ["pizza", "pasta", "pastaria", "risotto","trattoria", "ristorante", "cucina", "corso", "italiano", "siciliano", "toscano", "romano", "saltoro", "anthony", "russo", "marino", "bruno", "rossi", "esposito", "gallo", "caruso", "rizzo", "greco", "giordano", "gabagool", "tony", "soprano"]

# cleaning the names to ease NLP processing
def clean_text(text):
  text = text.lower()
  text = text.strip()
  text = text.replace("[^\w\s]", "")
  return text

# NLP function
def is_ital_restaurant(restaurant_name):
  cleaned_name = clean_text(restaurant_name)
  doc = nlp(cleaned_name)
  for token in doc:
      if token.text in ital_keywords:
          return True
  return False

# applying functions and final filter
restaurant_df["cleaned_name"] = restaurant_df["Name"].apply(clean_text)
restaurant_df["ital_check"] = restaurant_df["cleaned_name"].apply(is_ital_restaurant)

ital_restaurant_df = restaurant_df[restaurant_df["ital_check"]]

# extra filter to ensure only locations in the city of Seattle
# (this data set covers the full county)
ital_restaurant_df["city_name"] = ital_restaurant_df["City"].apply(clean_text)
ital_restaurant_df = ital_restaurant_df[ital_restaurant_df["city_name"]== "seattle"]

print(ital_restaurant_df.head(5))
```


# Geo-locating pizza venues and mapping response windows
Per our research, the average restaurant delivery time is 28.18 minutes for this type of food (https://doi.org/10.1016/j.trip.2023.100891). In the city of Seattle specifically, delivery drivers indicate that their average range driving for a delivery is 5 miles (https://www.ridesharingforum.com/t/comparing-ubereats-doordash-and-postmates-in-seattle-wa/292). This finding is fairly comparable to other data from similar cities like Atlanta, GA (https://opposite-lock.com/topic/5441/the-data-of-driving-for-doordash)
```{python}
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point, Polygon
from geopy.geocoders import Nominatim

# conversion of restaurants to geospatial data
rest_gdf = gpd.GeoDataFrame(ital_restaurant_df, geometry= gpd.points_from_xy(ital_restaurant_df["Longitude"], ital_restaurant_df["Latitude"]))

# matching the geospatial reference from earlier
rest_gdf = rest_gdf.set_crs("EPSG:3857", allow_override= True)

# pulling in our census tract-based data as a basemap
seatt_basemap = agg_calls_census_merged.copy()
seatt_basemap = seatt_basemap.set_crs("EPSG:3857", allow_override= True)

# importing police station locations
police_locations = pd.read_csv("Police_Station_Locations_in_King_County___kcp_loc_point.csv")
police_locations = police_locations[police_locations["CITY"] == "Seattle"]
police_locations["full_address"]= police_locations["ADDRESS"] + ", " + police_locations["CITY"] + ", " + police_locations["STATE_NAME"] + ", " + police_locations["ZIPCODE"].astype(str)

# ChatGPT suggested way to convert addresses to point data
def geocode_address(address):
    geolocator = Nominatim(user_agent= "your_app_name")
    location = geolocator.geocode(address)
    if location is None:
        return None
    else:
        return location.latitude, location.longitude

police_locations[["latitude", "longitude"]] = police_locations["full_address"].apply(geocode_address).apply(pd.Series)
police_gdf = gpd.GeoDataFrame(
    police_locations,
    geometry= gpd.points_from_xy(police_locations["longitude"], police_locations["latitude"])
)
police_gdf = police_gdf.set_crs("EPSG:3857", allow_override= True)

# ensuring that our observations are just within our basemap area
basemap_dissolved = seatt_basemap.dissolve()
rest_gdf["within_basemap"] = rest_gdf.geometry.within(basemap_dissolved.geometry[0])
rest_gdf_filtered = rest_gdf[rest_gdf["within_basemap"]]


# Mapping out our locations
fig, ax = plt.subplots(figsize=(10, 10))

# Plot the Seattle map as the base layer
seatt_basemap.plot(ax= ax, color= "lightgray", edgecolor= "black", alpha= 0.7)

# Plot the restaurant points as circles with the specified radius
rest_gdf_filtered.plot(ax= ax, color= "#800000", markersize= 20, label= "Restaurants")
police_gdf.plot(ax= ax, color= "#000080", marker= "*", markersize= 20, label= "Police Stations")

ax.legend(title= "Key", loc= "upper left", frameon= True, prop= {'size': 12})
ax.set_title("Seattle Pizza Restaurants and Police Station Point Map")
ax.axis("off")
plt.show()
```

# building our delivery range/police precinct ranges
```{python}
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point, Polygon

# buffering around our two points sets
buff_police = police_gdf.copy()
buff_rest = rest_gdf_filtered.copy()

# 0.02 and 0.01 below represent 6 and 3 miles in decimal degrees
buff_police["geometry"] = buff_police["geometry"].buffer(0.02)
buff_rest["geometry"] = buff_rest["geometry"].buffer(0.01)

# Mapping out our locations
fig, ax = plt.subplots(figsize=(10, 10))

# Plot the Seattle map as the base layer
seatt_basemap.plot(ax= ax, color= "lightgray", edgecolor= "black", alpha= 0.7)

# Plot both sets of points as circles with the specified radius
# Restaurants
rest_gdf_filtered.plot(ax= ax, color= "#800000", markersize= 20, label= "Restaurants")
buff_rest.plot(ax= ax, color= "#800000", alpha= 0.3, edgecolor= "#800000", linewidth=1 )

# Police
police_gdf.plot(ax= ax, color= "#000080", marker= "*", markersize= 20, label= "Police Stations")
buff_police.plot(ax= ax, color= "#000080", alpha= 0.3, edgecolor= "#000080", linewidth=1 )

# General legend
ax.legend(title= "Key", loc= "upper left", frameon= True, prop={"size": 12})
ax.set_title("Seattle Pizza Restaurants with 3-mile Delivery Radius and Police with 6 mile range")
ax.axis("off")
plt.show()
```


# buffer area calulcations
```{python}
import geopandas as gpd

# Calculate buffered areas
buff_police["area"] = buff_police["geometry"].area
buff_rest["area"] = buff_rest["geometry"].area
seatt_basemap["area"] = seatt_basemap["geometry"].area

# Calculate total buffer area
total_buffer_area = buff_police["area"].sum() + buff_rest["area"].sum()

# Calculate uncovered area (assuming seatt_basemap has an area property)
uncovered_area = seatt_basemap.area - total_buffer_area
uncovered_area = (uncovered_area * 1609.34)

total_uncovered_area = (abs(uncovered_area.sum()))/1000
print(f"Total Uncovered Area: {total_uncovered_area:.2f} sq miles")
```

# Write-Up

Research Question 

Our research question was whether or not police response times are truly slower than pizza delivery times, as the old American social trope suggests. We also sought to evaluate if the recovered response times are reasonable and explore what that discrepancy could mean to policymakers. 

Approach/Methodology 

We acquired a large dataset on police call responses from the Vera Institute of Justice, and chose to focus on one of their sample cities, Seattle. The initial Seattle dataset contained approximately 5 million observations, with each observation one call placed to the police, the initial purpose, time, and resolution. Our analysis narrowed in on approximately 2 million observations by focusing on just 911 calls as these are emergencies which would require the swift action we wished to observe. Other observations in the data included non-emergency police contact, and officers that were already on-site to respond to a crime. These aren’t as relevant as 911 calls because they either are self-selected by the public as being non-emergencies or are a result of the effectiveness of police patrols respectively. The main variables we focused on are the response time in minutes, the initial call disposition, latitude, and longitude. 

We then were able to use census tract data to identify from which census tract a call originated via spatial merge. We used this to group by census tract and align relevant demographic information from each tract. Initially, we explored on too granular a level with our analysis to find any discernable differences among different census tracts and response times. However, after reviewing the census summary data and noting that Seattle has a far greater proportion of the city identifying as White, we concluded that bifurcating the data along the intracity average ‘whiteness’ could serve as a comparison of areas that are whiter than average vs more diverse than average in terms of police response times.  

We also used Seattle food-based business license data to evaluate the distribution of pizza restaurants in the city, creating that dataset by using NLP to filter & identify Italian restaurants by training Spacy on Italian names & restaurant naming styles. We geolocated the listed business addresses and joined these locations with the existing police precinct locations to create a buffered plot that shows the potential delivery area coverage of pizza restaurants throughout the city and of the police precincts. We had some difficulty converting the data to the appropriate CRS at one point so that we could plot the desired 3-mile radius around pizza restaurants and 6 miles around police stations, so approximations for the relevant units (decimal degrees) had to be used.  

Plots 

![This plot shows the different average response times by call types that exist between more diverse and less diverse census tracts. A red line is displayed to show the overall average response time within Seattle.](pictures/race_chart_line.png){width=80%}

![Our shiny app displays the differences in call response time in Seattle overall with a static plot, and compares that to a dynamic plot where a user can filter based upon the call type. Overall average response time and within call type averages are also displayed for user's reference."](pictures/shiny_app.png){width=80%}

![Our initial plot shows most police stations are located in the southern and central parts of the city. Pizza restaraunts primarily located in the central and northern parts of the city.](pictures/pizza_and_police_map.png){width=40%}

![Our second plot buffers each location to give perspective on the potential coverage of each of the respective entities within a 30 minute drive. We assumed 3 miles for pizza and 6 miles for a police officer.](pictures/pizza_police_radius.png){width=40%}

Policy Implications 

Our findings helped show a link between slower response times and police precinct placement in Seattle. Additionally, we found that more diverse neighborhoods had faster response times. Through comparison with pizza restaurants, we sought to show that emergency police response times generally are too slow. Policymakers should think carefully about precinct placement throughout municipalities to minimize these differences and reduce police response times. An additional policy implication from these results is that emergency response is hampered by calls to police that do not necessarily require an emergency response, policymakers could also consider public awareness campaigns so residents are more aware of other city services that can address road safety concerns, downed trees, etc., concerns that are currently handled by police. 

Future Work 

One of the key findings that we had was that the police response time lags significantly depending on the type of call they believe they are responding to, however our data also indicates that the initial call characterization and the final report given by the responding officer often differ, and in many cases differ in matters of life & death. There were cases where, for example, a call was initially labeled as a noise complaint, but the final report indicated that it was a case of domestic violence. Given that the latter type of incident can be fatal, further research should explore the frequency of this consequential change.  