---
title: "Testing the Data"
format: html
---

# initial data testing
```{python}
import pandas as pd
import zipfile

data_path = r"calls_all.csv.zip"
with zipfile.ZipFile(data_path, "r") as zip_ref:
    with zip_ref.open("calls_all.csv") as file:
        new_data = pd.read_csv(file)
```

# Data Structure
```{python}
new_data.shape
new_data.columns
new_data["Event Clearance Description"].unique()
new_data["Call Type"].unique()
new_data["Priority"].unique()
new_data["Initial Call Type"].nunique()
new_data["Final Call Type"].nunique()

new_data["Precinct"].unique()
len(new_data[new_data["Precinct"]=="UNKNOWN"])

new_data["Sector"].unique()
new_data["Beat"].unique()

```

5,926,156 unique calls (rows)
13 colums for each observation

Event Clearance Description appears to indicate what an officer did to resolve a phone call.

Call Type is nominal but unclear what the dispositions are for some.
Priority is a quantitative variable from 1-9. has 712 nas also.

Initial Call Type is very messy and we could recode. lots of different dispositions that we could aggregate into common categories. Same with Final Call Type. 326 and 436 respectively unique values.

West, Southwest, East, South, North, and Unknown are the only values for Precinct. There's 52,381 Unknown entries - not a lot.

Sector has a number of values that seem to be names? Need to investigate what this means if a data dictionary is handy.

Quite a few beats also.

```{python}
import altair as alt
grouping_911 = new_data.groupby("Call Type").size().reset_index(name="count")

alt.Chart(grouping_911).mark_bar().encode(
    alt.X("Call Type"),
    alt.Y("count")
).properties(title="Number of Each Call Types in Seattle")

```


# testing date-time range
```{python}
new_data["Original Time Queued"] = pd.to_datetime(new_data["Original Time Queued"], 
    format="%m/%d/%Y %I:%M:%S %p", 
    utc=True)

earliest_time = new_data["Original Time Queued"].min()
latest_time = new_data["Original Time Queued"].max()

print(f"The earliest time is {earliest_time}, the latest is {latest_time}")
```

# initial data clean up
```{python}
import pandas as pd

# removing rows with blurred lat-long that is not usable
new_data = new_data[new_data["Blurred_Longitude"] != -1]
new_data = new_data[new_data["Blurred_Longitude"] != 0]

# removing rows without an arrival time
new_data = new_data.dropna(subset= ["Arrived Time"])

# filtering down to the list of beats that are in Seattle city limits (per their website)
seattle_beats = pd.read_csv("Seattle_PD_beats.csv")
seattle_beats_list = seattle_beats["beat"].unique()

new_data = new_data[new_data["Beat"].isin(seattle_beats_list)]
```

# creating new data - time differences
```{python}
import pandas as pd

# time difference calculation
new_data["Arrived Time"] = pd.to_datetime(
    new_data["Arrived Time"], 
    format="%m/%d/%Y %I:%M:%S %p", 
    utc=True
)

new_data["Original Time Queued"] = pd.to_datetime(new_data["Original Time Queued"], format="%m/%d/%Y %I:%M:%S %p", utc= True)

new_data["Time_Diff_Minutes"] = (new_data["Arrived Time"] - new_data["Original Time Queued"]).dt.total_seconds()/60
new_data["Time_Diff_Minutes"] = new_data["Time_Diff_Minutes"].fillna(0)
```

# checking general average response time
```{python}
average_response_time = new_data["Time_Diff_Minutes"].mean()
print(f"average response time overall is {average_response_time: .2f} minutes")
```

# checking by call type
```{python}
grouping_911 = new_data.groupby("Call Type")
emergency_response_time = grouping_911.get_group("911")["Time_Diff_Minutes"].mean()
print(f"average response time for 911 calls is {emergency_response_time: .2f} minutes")
```

# average response time by location

```{python}
import altair as alt
#look at which beats have the most calls
beat_counts=new_data.groupby("Beat").size().sort_values(ascending=False).reset_index(name="count")

#create a new df looking at avg response time by beats
response_beat=new_data.groupby("Beat")["Time_Diff_Minutes"].mean(

).sort_values(
    ascending=False).reset_index()
#create a chart
alt.Chart(response_beat).mark_bar().encode(
    alt.X("Beat",sort=response_beat["Beat"].tolist()),
    alt.Y("Time_Diff_Minutes",title="Total Response Time")
).properties(title="Overall Response Time By Beat")

#now look at just 911 calls
calls_911=new_data[new_data["Call Type"]=="911"]
#groupby beat to see what avg response time is per beat
response_beat_911=calls_911.groupby("Beat")["Time_Diff_Minutes"].mean(

).sort_values(
    ascending=False).reset_index()
#going to need this later to create a chart displaying the 
#avg response time
emergency_response_time_df = pd.DataFrame(
    {'emergency_response_time': [emergency_response_time]})

#create initial chart for 911 calls by beat, and response time
chart_beat_911=alt.Chart(response_beat_911).mark_bar().encode(
    alt.X("Beat",sort=response_beat_911["Beat"].tolist()),
    alt.Y("Time_Diff_Minutes",title="Response Time (Minutes)")
).properties(title="Response Time By Beat for 911 Calls")

#create the horizontal line representing avg response time for 
#911 calls regardless of beat
avg_response_line = alt.Chart(emergency_response_time_df).mark_rule(color="red").encode(
    y=alt.Y("emergency_response_time:Q", title="Average Response Time")  # Quantitative data type
)

chart_w_average=chart_beat_911+avg_response_line
chart_w_average.show()
```

Final Chart shows that B1, B2, B3, U3, J2, J3, J1, N2, D3, U1, Q2, N3, Q1,U2, C2, D2, L3, L2 all perform worse on response time than the average.


# average response time by call type


# grouping initial call type for 911 calls 
There are 249 unique values for this. It would be nice if we could group in a way that provides some clarity about what kind of calls are receiving fast response and which ones are not. Then we can evaluate if its proportionate to the crime, and we could provide granularity. Maybe even use to filter a map on a dashboard by the response times based upon our new call types
```{python}
unique_call_types=calls_911["Initial Call Type"].unique()

calls_911.groupby("Initial Call Type").size().sort_values(ascending=False).reset_index(name="count").head(20)

#create new initial call category
import numpy as np

# Define conditions and corresponding labels
conditions = [
    calls_911["Initial Call Type"].str.contains("DV", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("narcotics", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("Theft|property|Burg|Robbery|Trespass|Arson", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("ASLT|fight|bomb|shots", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("Nuisance|Noise", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("suicide|Wellness Check|Welfare Check|Overdose|emotional crisis", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("Suspicious", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("MVC|Traffic|DUI", case=False, na=False),
    calls_911["Initial Call Type"].str.contains("lewd|Rape|Sex", case=False, na=False)
]

labels = [
    "domestic violence",
    "drug crime",
    "property crime",
    "violent crime",
    "noise complaint",
    "wellness check",
    "community suspicion",
    "traffic safety",
    "sex crime"
]

# Apply transformations using np.select
calls_911["Call Category"] = np.select(conditions, labels, default="other")

check_call_cat=calls_911[["Call Category","Initial Call Type"]]


# Calculate Q1, Q3, and IQR
q1 = calls_911['Time_Diff_Minutes'].quantile(0.25)
q3 = calls_911['Time_Diff_Minutes'].quantile(0.75)
iqr = q3 - q1

# Define the acceptable range
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

print(f"Lower bound: {lower_bound}, Upper bound: {upper_bound}")

filtered_calls_911 = calls_911[
    (calls_911['Time_Diff_Minutes'] >= lower_bound) &
    (calls_911['Time_Diff_Minutes'] <= upper_bound)
]

print(f"Original data size: {len(calls_911)}, Filtered data size: {len(filtered_calls_911)}")

call_category_chart = alt.Chart(filtered_calls_911).mark_boxplot().encode(
    x='Call Category:N',
    y='Time_Diff_Minutes:Q'
)
call_category_chart

call_category_chart2=alt.Chart(calls_911).mark_bar().encode(
    x='Call Category:N',
    y=alt.X('mean(Time_Diff_Minutes):Q',title="Average Response Time(Minutes)")
).properties(title="Average Police Response Time by Call Category in Seattle")

call_category_chart2.show()
#Now I want to transpose this over census, and geographic data
```


# NLP Extra Credit - Name Recognition
Since our dataset from the city of Seattle on food-based businesses does not come with any kind of attribution, we will be using Natural Language Processing to isolate the food venues likely to serve pizzas. 

```{python}
import spacy
import pandas as pd

# setup steps
restaurant_df = pd.read_csv("Businesses_Food-King_County_20241024.csv")
nlp = spacy.load("it_core_news_sm")
# common pizza/Italian restaurant key words and top 20 most common Italian last names (source: https://surnamesinitaly.com/italian-surnames-usa/)
ital_keywords = ["pizza", "pasta", "risotto","trattoria", "ristorante", "cucina", "italiano", "siciliano", "toscano", "romano", "saltoro", "russo", "marino", "bruno", "rossi", "esposito", "gallo", "caruso", "rizzo", "greco", "giordano", "gabagool"]

# cleaning the names to ease NLP processing
def clean_text(text):
  text = text.lower()
  text = text.strip()
  text = text.replace("[^\w\s]", "")
  return text

# NLP function
def is_ital_restaurant(restaurant_name):
  cleaned_name = clean_text(restaurant_name)
  doc = nlp(cleaned_name)
  for token in doc:
      if token.text in ital_keywords:
          return True
  return False

# applying functions and final filter
restaurant_df["cleaned_name"] = restaurant_df["Name"].apply(clean_text)
restaurant_df["ital_check"] = restaurant_df["cleaned_name"].apply(is_ital_restaurant)

ital_restaurant_df = restaurant_df[restaurant_df["ital_check"]]

# extra filter to ensure only Seattle city proper
ital_restaurant_df["city_name"] = ital_restaurant_df["City"].apply(clean_text)
ital_restaurant_df = ital_restaurant_df[ital_restaurant_df["city_name"]== "seattle"]

print(ital_restaurant_df.head(5))
```